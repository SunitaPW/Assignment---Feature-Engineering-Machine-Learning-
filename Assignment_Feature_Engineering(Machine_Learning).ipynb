{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Answers**"
      ],
      "metadata": {
        "id": "iPmhTc-bdrvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 :- What is a parameter?"
      ],
      "metadata": {
        "id": "o013KMKFd0bQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "A **Parameter** in machine learning is a configuration variable that is internal to the model and is learned from the training data.\n",
        "  \n",
        "These are the components of a machine learning model that are estimated or tuned during the training process to optimize the model's performance."
      ],
      "metadata": {
        "id": "Wvy-yk71eZ87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2 :- What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "U1vSOnpaewQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### **Correlaion :-** Correlation is a statistical measure that describes/express the extent and direction of the linear relationship between two variables. It indicates how changes in one variable are associated with changes in another variable.\n",
        "\n",
        "### **Negative Correlation :-** A negative correlation is a relationship between two variables that move in opposite directions, means that as one variable increases, the other variable tends to decrease."
      ],
      "metadata": {
        "id": "xNG_J3Kye0b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3 :- Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "e0hVrbhKf30t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### **Machine Learning :-**\n",
        "Machine learning is when both data and output are run on a computer to create a program that can then be used in traditional programming.\n",
        "\n",
        "ML (Machine Learning) is a branch of artificial intelligence that allow system to learn and improve from experience without being explicitly programmed.\n",
        "\n",
        "\n",
        "### These main components plays a critical role in the machine learning workflow :-\n",
        "\n",
        "**1) Data :-**\n",
        "- Data is the foundational component of machine learning, It includes input features and target variables and Can be categorized into:\n",
        "  -- Training data\n",
        "  -- Validation data\n",
        "  -- Testing data\n",
        "- Quality and quantity of data significantly impact model performance.\n",
        "\n",
        "**2) Features :-**\n",
        "- Individual measurable properties or characteristics of a phenomenon is used as input variables for machine learning models, Feature selection and engineering are crucial steps can be numerical, categorical, or derived from raw data.\n",
        "\n",
        "**3) Algorithms :-**\n",
        "- Mathematical models that learn patterns from data, there are different types serve various purposes:\n",
        "  - Supervised Learning Algorithms\n",
        "  - Unsupervised Learning Algorithms\n",
        "  - Semi-Supervised Learning Algorithms\n",
        "  - Reinforcement Learning Algorithms\n",
        "\n",
        "**4) Model :-**\n",
        "- The output of the machine learning algorithm represents the learned patterns and relationships used to make predictions or decisions requires training, validation, and testing.\n",
        "\n",
        "**5) Loss Function :-**\n",
        "- It measures the difference between predicted and actual values helps optimize the model's performance guides the learning process by quantifying model errors, Examples include :- Mean Squared Error, Cross-Entropy Loss.\n",
        "\n",
        "**6) Optimization Techniques :-**\n",
        "- Methods to minimize the loss function adjust model parameters to improve performance and the common techniques:\n",
        "  -- Gradient Descent\n",
        "  -- Stochastic Gradient Descent\n",
        "  -- Adam Optimizer\n",
        "  -- RMSprop\n",
        "\n",
        "**7) Hyperparameters :-**\n",
        "- It configuration settings external to the model and set before the learning process begins ontrol the learning algorithm's behavior.\n",
        "- Examples :- learning rate, number of hidden layers, regularization strength.\n",
        "\n",
        "**8) Evaluation Metrics :-**\n",
        "- The Quantitative measures to assess model performance and different metrics for various types of problems:\n",
        "  -- Classification: Accuracy, Precision, Recall, F1-Score\n",
        "  -- Regression: Mean Squared Error, R-squared\n",
        "  -- Clustering: Silhouette Score, Adjusted Rand Index\n",
        "\n",
        "**9) Training Process :-**\n",
        "- It Iterative process of learning from data and Involves:\n",
        "  -- Feeding data to the algorithm\n",
        "  -- Adjusting model parameters\n",
        "  -- Minimizing loss function\n",
        "  -- Improving predictive performance\n",
        "\n",
        "**10) Prediction/Inference :-**\n",
        "- The final stage where the trained model makes predictions and applied to new, unseen data and transforms learned patterns into actionable insights.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dSRruq4f60D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4 :- How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "ySSQoOOmmVs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### Firstly in ML (Machine Learning), the **loss value** plays a critical role in evaluating the performance of a model. It quantifies how well or poorly/bad the model's predictions align with the actual target values.\n",
        "\n",
        "\n",
        "### Here are some points how the loss value helps in determining whether a model is good or not:\n",
        "\n",
        "**1) Indicates Model's Accuracy :-**\n",
        "\n",
        "- The **loss value** measures the difference between the predicted outputs and the actual values (ground truth).\n",
        "\n",
        "- A **lower loss value** generally indicates that the model's predictions are closer to the actual values, meaning the model is performing well.\n",
        "\n",
        "- A **higher loss value** suggests that the model is not predicting accurately, meaning it has a poor fit.\n",
        "\n",
        "**2) Helps with Optimization :-**\n",
        "\n",
        "- During training, the model tries to minimize the loss function by adjusting its parameters (weights and biases). This process is called **optimization**.\n",
        "\n",
        "- If the loss decreases over time (i.e., the loss value becomes smaller), it indicates that the model is improving and learning from the data.\n",
        "\n",
        "- If the loss does not decrease, or starts increasing, it could suggest that the model is not learning properly or is overfitting/underfitting.\n",
        "\n",
        "**3) Comparison Between Different Models :-**\n",
        "\n",
        "- When comparing different models or algorithms, the loss value can help determine which model performs best.\n",
        "\n",
        "- A **model with a consistently lower loss value** can be considered better at making predictions than one with a higher loss value.\n",
        "\n",
        "**4) Model Evaluation :-**\n",
        "\n",
        "- The loss value gives an objective way to evaluate a model. For example, a **regression model** might use **Mean Squared Error (MSE)**, and a **classification model** might use **Cross-Entropy Loss**.\n",
        "\n",
        "- By tracking the loss value on the **training set** and **validation set**, you can assess whether the model is overfitting (low training loss, high validation loss) or underfitting (high training loss).\n",
        "\n",
        "**5) Direction for Improvement :-**\n",
        "\n",
        "- If the loss is too high, it suggests areas where the model might be making large errors, providing clues about features to improve or whether more training data is needed.\n",
        "\n",
        "- The loss value provides feedback on what parts of the model need adjustment, such as changing hyperparameters, using different algorithms, or preprocessing data better.\n",
        "\n",
        "Hence, a good model typically has a lower loss value, indicating it makes accurate predictions. Conversely, a high loss suggests the model is not performing well and requires further improvement."
      ],
      "metadata": {
        "id": "iUJfAtVBmVpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 :- What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "u2fm7tiovdjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### **Continuous Variables :-**\n",
        "\n",
        "- Continuous variables are quantitative variables that can take any value within a certain range or interval. These variables are numerical and can represent measurements that may have an infinite number of possible values within a specific range.\n",
        "\n",
        "- It can take any value (including decimals/floats) within a certain range.\n",
        "\n",
        "- It can be discrete or fractional depending on the precision of the measurement.\n",
        "\n",
        "- The Statistical Analysis in Continuous variable is Mean, median, standard deviation, correlation.\n",
        "\n",
        "- It often represent measurements or counts that are expressed in units (like height, weight, temperature, distance, time, etc).\n",
        "\n",
        "\n",
        "### **Categorical Variables :-**\n",
        "\n",
        "- Categorical variable are qualitative variables that represent categories or labels. These variables have a limited number of distinct values or levels, and each value represents a category or group rather than a numerical measurement.\n",
        "\n",
        "- It can take on finite values, and these values do not have an inherent order (in the case of nominal data) or have a natural ranking (in the case of ordinal data).\n",
        "\n",
        "- Categorical variables cannot be directly involved in mathematical operations like continuous variables. They are typically represented by labels or codes.\n",
        "\n",
        "- The Statistical Analysis in Categorical variable is Mode, frequency counts, chi-squared tests.\n",
        "\n",
        "- It represent distinct groups or categories (e.g., color, gender, type of product).\n"
      ],
      "metadata": {
        "id": "KDWnGEVevddU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6 :- How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "3ZgolhiHxFSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### Handling categorical variables in machine learning is crucial for ensuring that algorithms can effectively process and interpret the data. Categorical variables are non-numeric data types (examples are :- \"red\", \"blue\", \"green\" or \"male\", \"female\") that need to be transformed into a numerical format for most machine learning models.\n",
        "\n",
        "### Below mentions points are common techniques to handle categorical variables :-\n",
        "\n",
        "**1) Label Encoding :-**\n",
        "\n",
        "- - This technique assigns a unique integer value to each category. For example, for a column with categories ('red', 'blue', 'green'), it might assign 0 to red, 1 to blue, and 2 to green.\n",
        "\n",
        "- - The label encoding is generally used when the categorical variable has an ordinal relationship.\n",
        "\n",
        "- **Disadvantage :-** For non-ordinal categories, label encoding may introduce artificial ordinality, which can mislead certain models (example :- linear regression or tree-based models) into thinking the higher numbers are more significant.\n",
        "\n",
        "\n",
        "**2) One-Hot Encoding :-**\n",
        "\n",
        "- - One-hot encoding creates binary columns for each category. For example, for a column with the categories ('red', 'blue', 'green'), it will create three binary columns: red, blue, and green. A row containing 'red' will be represented as [1, 0, 0].\n",
        "\n",
        "- - This technique is ideal for nominal (non-ordinal) categorical variables, where no inherent ordering exists between categories.\n",
        "\n",
        "- - **Disadvantage :-** It can lead to high-dimensionality, especially when the categorical feature has many unique values (example, a column representing zip codes or country names), leading to sparse matrices.\n",
        "\n",
        "**3) Ordinal Encoding :-**\n",
        "\n",
        "- - Similar to label encoding, but it's specifically used for ordinal categorical variables (those that have a defined order or ranking, such as \"low\", \"medium\", \"high\"). The categories are replaced with integers according to their order.\n",
        "\n",
        "- - This is the best method for categorical features that have a natural ranking (example, 1 for low, 2 for medium, 3 for high).\n",
        "\n",
        "- - **Disadvantage :-** It assumes that the difference between adjacent categories is equal, which might not always be true.\n",
        "\n",
        "**4) Target Encoding (Mean Encoding) :-**\n",
        "\n",
        "- - This method involves replacing each category with the mean of the target variable for that category. For example, for a categorical column such as \"color\", you would compute the mean target value for each color and replace the color with its corresponding mean target value.\n",
        "\n",
        "- - Target encoding is often used when dealing with high-cardinality categorical variables. It works well with models like linear regression or gradient boosting.\n",
        "\n",
        "- - **Disadvantages :-** It can cause overfitting, especially if the categorical variable has many unique values, because it might overemphasize the specific relationships between the categories and the target."
      ],
      "metadata": {
        "id": "X8fjV9rfxFN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7 :- What do you mean by training and testing a dataset ?\n"
      ],
      "metadata": {
        "id": "WI-Umc7y_d8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### Training and testing a dataset are fundamental concepts in machine learning, and they refer to how a machine learning model is developed, validated, and evaluated.\n",
        "\n",
        "- Training is about learning from data, while testing is about evaluating how well the model generalizes to unseen data.\n",
        "\n",
        "- The model should be trained on one set of data and tested on a different set to avoid overfitting and ensure it can generalize well to new inputs.\n",
        "\n",
        "### **Training a Dataset :-**\n",
        "\n",
        "- Training a dataset involves using the data to build a model. This process entails feeding the data (inputs and corresponding outputs) into an algorithm that learns patterns, relationships, or features from the data. The model adjusts its internal parameters to minimize the error or loss function during training.\n",
        "\n",
        "- The goal of training is for the model to learn the relationships between the input features (independent variables) and the target output (dependent variable), so it can generalize and make accurate predictions on new, unseen data.\n",
        "\n",
        "- Training set used to train the model and adjust its parameters.\n",
        "\n",
        "### **Testing a Dataset :-**\n",
        "\n",
        "- Testing a dataset involves evaluating the performance of the trained model using a separate set of data that the model has never seen before. This testing data is used to assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "- The purpose of testing is to evaluate the model's effectiveness in real-world scenarios where the model will encounter new data.\n",
        "\n",
        "- The test data helps to detect issues such as overfitting (where the model performs well on training data but poorly on new data) or underfitting (where the model does not capture the underlying patterns even in the training data).\n",
        "\n",
        "- Test set used to evaluate the model's performance after training."
      ],
      "metadata": {
        "id": "V8vi0IyN_d4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8 :- What is sklearn.preprocessing ?"
      ],
      "metadata": {
        "id": "i9y-h_sG_d1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### sklearn.preprocessing is a module within the scikit-learn library in Python, which provides a set of functions and classes for preprocessing data before feeding it into machine learning algorithms. Preprocessing refers to transforming raw data into a format that is suitable for modeling and analysis. In machine learning, this step is crucial for improving model accuracy and performance.\n",
        "\n",
        "The sklearn.preprocessing module contains tools for various data transformation tasks, such as scaling features, encoding categorical variables, and handling missing values.\n",
        "\n",
        "### There are some common Functions in sklearn.preprocessing :-\n",
        "\n",
        "**1) MinMaxScaler :-**\n",
        "\n",
        "- In the MinMaxScaler the purpose of Scales features to a specific range, usually between 0 and 1.\n",
        "\n",
        "- This is useful when you want to normalize features without losing the relationships between the data points.\n",
        "\n",
        "- It scales each feature by subtracting the minimum value and dividing by the range (max-min).\n",
        "\n",
        "**2) OneHotEncoder :-**\n",
        "\n",
        "- This converts categorical variables into a form that can be provided to machine learning algorithms, typically into one-hot encoded format (binary vectors).\n",
        "\n",
        "- This is used when you have categorical features with multiple classes.\n",
        "\n",
        "- It creates a binary column for each category and assigns a 1 or 0 depending on the presence of the category.\n",
        "\n",
        "**3) Normalizer :-**\n",
        "\n",
        "- Normalizes data such that each sample (row) has unit norm (i.e., the sum of squares of features equals 1).\n",
        "\n",
        "- Typically used when working with sparse data or for certain types of models like k-nearest neighbors.\n",
        "\n",
        "- It scales each row (sample) to have a unit norm, which means dividing each value in the row by the norm (L2 norm).\n",
        "\n",
        "\n",
        "### The sklearn.preprocessing is important for :-\n",
        "\n",
        "- Standardization and Scaling\n",
        "- Handling Categorical Data\n",
        "- Improving Model Accuracy\n",
        "- Handling Skewed Data"
      ],
      "metadata": {
        "id": "jw8z0VvP_dzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9 :- What is a Test set ?"
      ],
      "metadata": {
        "id": "tpbYI_Gl_dwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### A test set in machine learning refers to a subset of the dataset that is used solely for evaluating the performance of a trained model. It contains data that the model has never seen during the training phase, and it serves as a way to check how well the model generalizes to new, unseen data.\n",
        "\n",
        "### The purpose of test set is used to assess the accuracy and performance of the trained model.\n",
        "\n",
        "- The test set is a crucial part of the machine learning workflow.\n",
        "\n",
        "- It is used for model evaluation after training, helping to ensure that the model generalizes well to new, unseen data.\n",
        "\n",
        "- It should be kept separate from the training set to avoid any bias or overfitting during the training process.\n",
        "\n",
        "- The performance of the model on the test set is a reflection of its expected real-world performance.\n",
        "\n",
        "### Some key points of Test set are :-\n",
        "\n",
        "**1) Data Split :-** When preparing data for machine learning, the dataset is typically split into at least two parts:\n",
        "\n",
        "- **Training set :-** Used to train the model and fit the parameters.\n",
        "- **Test set :-** Used to evaluate the model's generalization ability. It is not involved in the model training process.\n",
        "\n",
        "**2) Evaluation Metrics :-** The model's predictions on the test set are compared to the actual values in the test set, and performance metrics.\n",
        "\n",
        "**3) Generalization :-** The test set helps to identify whether a model is overfitting or underfitting.\n",
        "\n",
        "- **Overfitting :-** If the model performs well on the training data but poorly on the test set, it may have memorized the training data and failed to generalize to new data.\n",
        "\n",
        "- **Underfitting :-** If the model performs poorly on both the training data and the test set, it may not have learned the underlying patterns in the data.\n",
        "\n",
        "**4) No Data Leakage :-** The test set must remain untouched during the training process. This means it should not be used in any way to influence the training of the model (no feature selection, hyperparameter tuning, or any adjustments based on the test set data). Using the test set inappropriately during training can lead to data leakage, which invalidates the evaluation results.\n",
        "\n",
        "**5) Real-World Performance Estimate :-** The performance of the model on the test set is often treated as an estimate of how the model will perform when deployed in real-world applications, where the model will encounter data it hasn't seen before."
      ],
      "metadata": {
        "id": "NZkszKms_dto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10 :- How do we split data for model fitting (training and testing) in Python ? How do you approach a Machine Learning problem ?"
      ],
      "metadata": {
        "id": "9MJhm4jK_dq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### In Python, the most common way to split data into training and testing sets is by using **train_test_split()** **from the sklearn.model_selection** module.\n",
        "\n",
        "### This function allows you to randomly split your dataset into training and testing sets.\n",
        "\n",
        "### Here's how you can do it :-"
      ],
      "metadata": {
        "id": "vg4C8K4QEwX1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Wv9Gh-PkdhMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe1a7ac-f0bf-49e2-a8cd-a42c703a535b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features:\n",
            " [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "Testing Features:\n",
            " [[3 4]]\n",
            "Training Labels:\n",
            " [0 0 0 1]\n",
            "Testing Labels:\n",
            " [1]\n"
          ]
        }
      ],
      "source": [
        "#  Import Required Libraries :-\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Created a \"X\" features and \"y\" labels :-\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "# Split the data into 80% training and 20% testing :-\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Testing Features:\\n\", X_test)\n",
        "print(\"Training Labels:\\n\", y_train)\n",
        "print(\"Testing Labels:\\n\", y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach a Machine Learning Problem :-\n",
        "\n",
        "Approaching a machine learning problem involves a series of structured steps. Here's a general process :-\n",
        "\n",
        "- **Define the problem :-** It understand the task (classification, regression, etc.).\n",
        "\n",
        "- **Collect and prepare data :-** It clean and preprocess the data.\n",
        "\n",
        "- **Split the data :-** It is use train_test_split() to create training and testing datasets.\n",
        "\n",
        "- **Choose a model :-** It select a suitable machine learning algorithm.\n",
        "Train the model: Fit the model on training data.\n",
        "\n",
        "- **Evaluate the model :-** It assess performance using evaluation metrics.\n",
        "\n",
        "- **Tune hyperparameters :-** It optimize the model for better performance.\n",
        "\n",
        "- **Deploy the model :-** It put the model into production.\n",
        "\n",
        "- **Monitor and maintain :-** It keep track of the model's performance and update when necessary."
      ],
      "metadata": {
        "id": "VXKeCftdFYCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Create and train the model :-\n",
        "# Initialize the model :-\n",
        "# Train the model :-\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions :-\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy :-\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adqPuao_A30H",
        "outputId": "b51ceba8-5e35-44b7-b510-8e644689d806"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11 :- Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "o8H2ydrcF3S-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is a crucial step in the data science workflow.\n",
        "\n",
        "EDA helps you to understand the dataset better, uncover underlying patterns, identify potential problems, and prepare the data for model training.\n",
        "\n",
        "\n",
        "### There are some point to understand why EDA is important before fitting a model :-\n",
        "\n",
        "**1) Data Understanding :-** Helps you comprehend the data, its structure, and the underlying relationships between features and target variables.\n",
        "\n",
        "**2) Data Quality :-** Identifies missing values, outliers, errors, and inconsistencies that could affect the model's performance.\n",
        "\n",
        "**3) Feature Engineering :-** Helps in creating new features and deciding which ones to include in the model.\n",
        "\n",
        "**4) Model Choice :-** Guides the selection of the right model and preprocessing techniques based on the data's characteristics.\n",
        "\n",
        "**5) Improved Performance :-** By identifying and addressing data issues (example :- outliers, missing values), you ensure that the model has the best chance of performing well.\n",
        "\n",
        "**6) Visual Insights :-** Visualizations make it easier to spot patterns, trends, and relationships in the data that may not be apparent through raw numbers alone.\n",
        "\n",
        "EDA is an essential step in the machine learning pipeline as it provides critical insights into the dataset, ensuring that you can make informed decisions when preparing the data and selecting the right model. Skipping this step could lead to poor model performance or misguided analysis, making it a key part of any data science project."
      ],
      "metadata": {
        "id": "QdoWjzGBGBLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12 :- What is correlation?"
      ],
      "metadata": {
        "id": "td9xs610HZz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "Correlation is a statistical measure that describes the degree to which two variables move in relation to each other.\n",
        "\n",
        "In simpler terms, it tells you whether and how strongly pairs of variables are related.\n",
        "\n",
        "It measures both the direction (positive or negative) and strength of the relationship between two variables.\n",
        "\n",
        "### **There are 3 Types of Correlation mention below :-**\n",
        "\n",
        "**1) Positive Correlation (Direct Relationship) :-**\n",
        "\n",
        "- When two variables move in the same direction, meaning as one variable increases, the other variable also increases.\n",
        "\n",
        "- **For Example :-** Height and weight of individuals generally have a positive correlation. As height increases, weight tends to increase as well.\n",
        "\n",
        "\n",
        "**2) Negative Correlation (Inverse Relationship) :-**\n",
        "\n",
        "- When two variables move in opposite directions, meaning as one variable increases, the other variable decreases.\n",
        "\n",
        "- **For Example :-** The amount of time spent studying and the number of hours spent watching TV might have a negative correlation. As study time increases, TV watching time decreases.\n",
        "\n",
        "\n",
        "**3) No Correlation (No Relationship) :-**\n",
        "\n",
        "- When there is no discernible relationship between the variables. Changes in one variable do not predict any consistent change in the other variable.\n",
        "\n",
        "- **For Example :-** Shoe size and intelligence have no correlation.\n",
        "\n",
        "\n",
        "### **There are some point mention below why Correlation is Important :-**\n",
        "\n",
        "**1) Identifying Relationships :-** It helps identify if two variables are related, and if they are, whether their relationship is direct or inverse.\n",
        "\n",
        "**2) Feature Engineering :-** Understanding correlation allows you to select the most relevant features and avoid including highly correlated ones that may introduce redundancy.\n",
        "\n",
        "**3) Data Preprocessing :-** Correlation helps you detect collinearity issues, which can be problematic in some machine learning models, leading to instability and overfitting.\n",
        "\n",
        "**4) Model Interpretability :-** Correlation provides insights into how variables interact, making it easier to understand and interpret the model's predictions.\n",
        "\n",
        "\n",
        "### **Here Correlation in Machine Learning :-**\n",
        "\n",
        "In machine learning, correlation plays a key role in feature selection and understanding relationships between variables:\n",
        "\n",
        "**1) Feature Selection :-** Highly correlated features may lead to multicollinearity in some models (like linear regression), which can affect the model's performance. If two features are highly correlated, one of them can be removed to reduce redundancy and improve the model's interpretability.\n",
        "\n",
        "**2) Data Insights :-** By examining the correlation between features and the target variable, you can gain insights into which features are most important in predicting the target and whether transformations (e.g., scaling, encoding) are needed.\n",
        "\n",
        "**3) Visualization :-** Correlation matrices or heatmaps are commonly used to visually show the strength of relationships between multiple variables in a dataset."
      ],
      "metadata": {
        "id": "FccKQSDxHdAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q13 :- What does negative correlation mean?"
      ],
      "metadata": {
        "id": "vtHIDMTbUsSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### Negative Correlation refers to a relationship between two variables in which one variable tends to decrease as the other increases,\n",
        "\n",
        "### Specifically in the case of :-\n",
        "- When one variable increases, the other variable decreases.\n",
        "- When one variable decreases, the other variable increases.\n",
        "\n",
        "### This kind of relationship is quantified by the correlation coefficient, which ranges from -1 to 1 :-\n",
        "\n",
        "- When a correlation coefficient close to -1 indicates a strong negative correlation.\n",
        "\n",
        "- When a correlation coefficient close to 0 indicates little to no correlation.\n",
        "\n",
        "- When a correlation coefficient close to 1 indicates a strong positive correlation.\n",
        "\n"
      ],
      "metadata": {
        "id": "eHT0zpqfU17b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q14 :- How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "dOz7S-vwVewU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### To find the correlation between variables in Python, We have one of the the most common approach is to use the Pandas library, which provides a built-in function to compute correlation matrices.\n",
        "\n",
        "- Pandas **corr()** function.\n",
        "\n",
        "- NumPy for Pearson Correlation **corrcoef()** function:.\n",
        "\n",
        "- Seaborn's Heatmap for Visualization.\n",
        "\n",
        "\n",
        "### Let's see using the above function, How we can find correlation :-"
      ],
      "metadata": {
        "id": "zjCKl3llVkeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Pandas \"corr()\" function and importing pandas library :-\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a data and convert that into DataFrame :-\n",
        "data = {'A': [1, 2, 3, 4, 5],\n",
        "        'B': [5, 4, 3, 2, 1],\n",
        "        'C': [2, 3, 4, 5, 6]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the correlation matrix :-\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n",
        "\n",
        "\n",
        "\"NOTE Here :-\"\n",
        "# 1.0 indicates a perfect positive correlation.\n",
        "# 1.0 indicates a perfect negative correlation.\n",
        "# 0.0 indicates no correlation.\n"
      ],
      "metadata": {
        "id": "RMkyNFcrFg7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "964a1922-d9fb-4210-d41e-b5a348487b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NumPy for Pearson Correlation \"corrcoef()\"\" function  (Measures linear relationship):-\n",
        "# Import numpy :-\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Checking Correlation between columns 'A' and 'B' :-\n",
        "correlation = np.corrcoef(df['A'], df['B'])[0, 1]\n",
        "\n",
        "print(f\"Correlation between A and B: {correlation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWdFttk8WwaW",
        "outputId": "349b4a66-b5e1-4045-f3aa-e2e9f80e5aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between A and B: -0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Seaborn's Heatmap for Visualization :-\n",
        "# Importing seaborn and matplotlib :-\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "MX0EDRupYLnF",
        "outputId": "73a0b060-715b-4768-e213-e054729c2f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q15 :- What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "V3dXhK5g6907"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "Causation refers to a relationship between two variables where one variable directly causes the other to change.\n",
        "\n",
        "In other words you can say, changes in one variable lead to changes in the other, typically through a cause-and-effect relationship.\n",
        "\n",
        "\n",
        "### Difference Between Correlation and Causation are :-\n",
        "\n",
        "### Correlation :-\n",
        "\n",
        "- Correlation refers to a statistical relationship between two variables, where they change in a predictable way together. However, correlation does not imply that one variable is causing the other to change.\n",
        "\n",
        "- The type of correlation can be positive, negative, or no correlation.\n",
        "\n",
        "- The cause and effect of correlation alone does not indicate that one variable is causing the other to change; it simply means that the two variables are related in some way.\n",
        "\n",
        "- The correlation is important because it can help identify patterns or associations, but it cannot determine cause-and-effect relationships.\n",
        "\n",
        "- Example :- A study finds a correlation between the number of hours students spend on social media and their academic performance and the **Observation :-** The study shows that students who spend more time on social media tend to have lower grades. **Interpretation :-** There is a negative correlation between time spent on social media and academic performance. As social media usage increases, grades tend to decrease.\n",
        "\n",
        "\n",
        "### Causation :-\n",
        "\n",
        "- Causation means that one variable directly influences the other, where a change in one variable directly causes a change in another.\n",
        "\n",
        "- The causation explicitly indicates a cause and effect relationship.\n",
        "\n",
        "- The Causation is important for understanding why something happens and making decisions based on those findings (e.g., public health policies, treatment strategies).\n",
        "\n",
        "- Example :- lack of sleep directly causes a decline in cognitive function and academic performance. Researchers observe that when students are sleep-deprived, their ability to concentrate, retain information, and perform well on tests decreases."
      ],
      "metadata": {
        "id": "wwtJIewD7B88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q16 :- What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "3eTArkdU9owN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### Optimizer helps the model learn by finding the best parameters that minimize the loss and improve predictions. It adjusts the parameters iteratively based on the gradients computed during the backpropagation step (in the case of neural networks).\n",
        "\n",
        "\n",
        "### There are few different types of Optimizers mention below :-\n",
        "\n",
        "**1) Stochastic Gradient Descent (SGD) :-**\n",
        "\n",
        "- It is one of the most basic and widely used optimization algorithms. It updates the model's weights by calculating the gradient of the loss function with respect to each parameter and moving in the direction of the negative gradient. In stochastic version, only one random training example is used to update the parameters at each step.\n",
        "\n",
        "- Example :- If you're training a linear regression model, you would use SGD to update the weights after each training example in the dataset, adjusting the model incrementally.\n",
        "\n",
        "- Advantages :-\n",
        "\n",
        "- - Can work well for large datasets.\n",
        "\n",
        "- - Simpler and computationally cheaper per update.\n",
        "\n",
        "- Disadvantages :-\n",
        "\n",
        "- - The updates are noisy due to using only a single data point, which can lead to slower convergence and potential instability.\n",
        "\n",
        "\n",
        "**2) Mini-Batch Gradient Descent :-**\n",
        "\n",
        "- Mini-Batch Gradient Descent is a variant of SGD where instead of using just one training example, a small batch of training examples (say 32, 64, or 128) is used to compute the gradient. This can speed up training and reduce the noise in the updates compared to pure SGD.\n",
        "\n",
        "- Example :- In neural networks, you might use mini-batches of 64 examples to update the weights after each batch instead of using just one example (as in SGD) or the whole dataset (as in batch gradient descent).\n",
        "\n",
        "- Advantages :-\n",
        "\n",
        "- - More stable than pure SGD.\n",
        "\n",
        "- - Faster convergence due to the use of mini-batches.\n",
        "\n",
        "- Disadvantages :-\n",
        "\n",
        "- - Still has some level of noise in the updates, although less than SGD.\n",
        "\n",
        "\n",
        "**3) Momentum :-**\n",
        "\n",
        "- Momentum is an extension of gradient descent that helps accelerate the algorithm in the relevant direction and dampen oscillations. It does so by considering the past gradients (history of updates), which helps to smooth out updates. The idea is to accumulate a \"velocity\" vector that helps the optimizer escape local minima or flat regions.\n",
        "\n",
        "- Example :- Momentum is often used in training deep neural networks, where the optimizer will keep a memory of past gradients to prevent updates from oscillating or getting stuck in flat regions of the loss surface.\n",
        "\n",
        "- Advantages :-\n",
        "\n",
        "- - Can converge faster than SGD.\n",
        "\n",
        "- - Helps escape local minima or saddle points.\n",
        "\n",
        "- Disadvantages :-\n",
        "\n",
        "- May overshoot the global minimum if not tuned carefully.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JCWIBynQ91in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q17 :- What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "mk9Ae1qFA8DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### **sklearn.linear_model** is a module in the Scikit-learn library (often abbreviated as sklearn) that provides a collection of algorithms and tools for linear models in machine learning. These models are used for regression and classification tasks, where the relationship between the dependent (target) variable and independent (predictor) variables is assumed to be linear or nearly linear.\n",
        "\n",
        "\n",
        "### Features of sklearn.linear_model mention below :-\n",
        "\n",
        "- **Simple and Efficient :-** Linear models are relatively simple but can work well for many problems, especially when the data is linearly separable or the relationship between features and the target is linear.\n",
        "\n",
        "- **Regularization Support :-** Many models in sklearn.linear_model (like Ridge, Lasso, and ElasticNet) support regularization, which helps to prevent overfitting, particularly in high-dimensional spaces.\n",
        "\n",
        "- **Scalable :-** Linear models scale well with larger datasets and are computationally less expensive compared to more complex models like neural networks.\n",
        "\n",
        "\n",
        "### There are few key Functions and Models in sklearn.linear_model :-\n",
        "\n",
        "**1) Linear Regression (LinearRegression) :-**\n",
        "\n",
        "- It is used for predicting a continuous target variable based on one or more input features (predictors).\n",
        "\n",
        "- The Use Case :- Predicting prices (e.g., house prices based on features like size, location, etc.), forecasting.\n",
        "\n",
        "**2) Logistic Regression (LogisticRegression) :-**\n",
        "\n",
        "- It is used for binary or multi-class classification problems where the target variable is categorical (e.g., 0 or 1, true or false).\n",
        "\n",
        "- The Use Case :- Classifying emails as spam or not, predicting disease presence (e.g., cancer or no cancer).\n",
        "\n",
        "**3) Ridge Regression (Ridge) :-**\n",
        "\n",
        "- A regularized version of linear regression that includes a penalty on the size of the coefficients to prevent overfitting. It is particularly useful when there is multicollinearity or when the model has many features.\n",
        "\n",
        "- The Use Case :- Predicting continuous values where there is high multicollinearity (e.g., predicting house prices using many correlated features).\n",
        "\n",
        "**4) Lasso Regression (Lasso) :-**\n",
        "\n",
        "- Similar to Ridge regression, but instead of using a squared penalty (like Ridge), it uses the absolute value of the coefficients, leading to sparsity in the solution. This means Lasso can perform feature selection by driving some coefficients to zero.\n",
        "\n",
        "- The Use Case :- When you want to identify and use only the most important features (e.g., in high-dimensional data).\n",
        "\n",
        "**5) ElasticNet Regression (ElasticNet) :-**\n",
        "\n",
        "- A combination of Ridge and Lasso regression, ElasticNet is useful when there are multiple correlated features. It uses both L1 and L2 regularization (Lasso and Ridge penalties).\n",
        "\n",
        "- The Use Case :- When dealing with highly correlated predictors, and when both feature selection and regularization are needed.\n",
        "\n",
        "**6) Passive Aggressive Classifier (PassiveAggressiveClassifier) :-**\n",
        "\n",
        "- A linear classifier designed for large-scale learning. It is called \"passive-aggressive\" because it aggressively updates the model only when the model's prediction is wrong (passive when correct).\n",
        "\n",
        "- The Use Case :- Online learning, where the model needs to be updated continuously with new data."
      ],
      "metadata": {
        "id": "22XJSVCtBBLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q18 :- What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "n7K7iAE7C5bG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "model.fit() is a method used to train a model using a given dataset. Specifically, it learns the patterns from the data by adjusting the internal parameters of the model (e.g., weights in linear models, decision boundaries in tree-based models) so that it can make accurate predictions on new, unseen data.\n",
        "\n",
        "The primary goal of model.fit() is to enable the model to learn from the training data so that it can generalize well and make predictions on new data.\n",
        "\n",
        "### What happens during model.fit() :-\n",
        "\n",
        "- **Model Initialization :-** The algorithm initializes the model's internal parameters (e.g., weights, coefficients, decision boundaries, etc.).\n",
        "\n",
        "- **Learning Process :-** The algorithm processes the training data, adjusts the parameters based on the data, and minimizes a loss function (e.g., mean squared error for regression, cross-entropy for classification).\n",
        "\n",
        "- **Convergence :-** The algorithm iteratively updates its parameters (often via optimization techniques like gradient descent) until it converges to the optimal parameters or reaches the specified maximum number of iterations.\n",
        "\n",
        "Once the model is trained (i.e., the parameters are learned), the model is ready to make predictions on new, unseen data using methods like model.predict().\n",
        "\n",
        "\n",
        "### Arguments for model.fit() are below :-\n",
        "\n",
        "### model.fit() requires at least two arguments :-\n",
        "\n",
        "- **X (features or predictors) :-** This is the input data that the model will use to learn the relationship between the features and the target variable. This is typically a 2D array (matrix) where each row represents an individual data point, and each column represents a feature.\n",
        "\n",
        "- - **Shape :-** (n_samples, n_features)\n",
        "\n",
        "- - **n_samples :-** Number of samples (data points).\n",
        "\n",
        "- - **n_features :-** Number of features (input variables).\n",
        "\n",
        "- **y (target or labels) :-** This is the target data or the output variable that the model is trying to predict or classify. This is a 1D array (vector) where each element corresponds to the target value for each data point in X.\n",
        "\n",
        "- - **Shape :-** (n_samples,)\n",
        "- - **n_samples :-** Same number as in X, representing the number of target values corresponding to the samples in X."
      ],
      "metadata": {
        "id": "Y1eJfO3DDBKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example :-\n",
        "\n",
        "# Import library :-\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load example dataset (Iris dataset) :-\n",
        "# \"X\" ---> Features (shape: n_samples, n_features)\n",
        "# \"y\" ---> Target labels (shape: n_samples,)\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize the model :-\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model using the training data :-\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "47QdCk43YsOP",
        "outputId": "84760e7c-b450-4d05-9041-d121f5dc900c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q19 :- What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "0bPUFxX4F7hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "model.predict() is a method used to make predictions on new, unseen data after a model has been trained. Once the model has learned from the training data through the fit() method, predict() allows you to use the trained model to generate predictions on new input data.\n",
        "\n",
        "### The predictions depend on the type of model being used. For example :-\n",
        "\n",
        "- In **regression models** (e.g., Linear Regression), the model predicts continuous numerical values.\n",
        "\n",
        "- In **classification models** (e.g., Logistic Regression, Decision Trees), the model predicts categorical labels (classes).\n",
        "\n",
        "- In some cases, the model might also return probabilities of each class (e.g., in Logistic Regression, you can predict the probability of belonging to a certain class).\n",
        "\n",
        "\n",
        "### Arguments for model.predict() mention below :-\n",
        "\n",
        "### The main argument required for model.predict() is :-\n",
        "\n",
        "- **X (Features or input data) :-** This is the input data for which you want to make predictions. It is a 2D array (matrix) where each row represents a sample, and each column represents a feature (or attribute).\n",
        "\n",
        "- - **Shape :-** (n_samples, n_features)\n",
        "\n",
        "- - **n_samples :-** Number of new data points (samples) for which predictions are required.\n",
        "\n",
        "- - **n_features :-** Number of features (input variables) the model expects to make predictions.\n",
        "\n",
        "The shape of X should match the shape of the data used to train the model (i.e., the number of features should be the same)."
      ],
      "metadata": {
        "id": "QutowkIZGECC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for a Classification Task (e.g., Logistic Regression) :-\n",
        "\n",
        "# import numpy :-\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load example dataset (Iris dataset) :-\n",
        "# \"X\" ----> Features (shape: n_samples, n_features)\n",
        "# \"y\" ----> Target labels (shape: n_samples,)\n",
        "\n",
        "data = load_iris()\n",
        "X_train = data.data\n",
        "y_train = data.target\n",
        "\n",
        "# Train the model :-\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction (e.g., a new flower with 4 features) :-\n",
        "X_new = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
        "\n",
        "# Predict the class of the new data :-\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(f\"Predicted class: {predictions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A65gJ13kFy0J",
        "outputId": "e9d7d5b4-f03a-4e1e-8423-c656449269e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for a Regression Task (e.g., Linear Regression) :-\n",
        "\n",
        "# Import :-\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Training data :-\n",
        "X_train = np.array([[1], [2], [3], [4]])\n",
        "y_train = np.array([1, 2, 3, 4])\n",
        "\n",
        "# Train the model :-\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction :-\n",
        "X_new = np.array([[5]])\n",
        "\n",
        "# Predict the value for the new data ;-\n",
        "prediction = model.predict(X_new)\n",
        "\n",
        "print(f\"Predicted value: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXkV-3w-HFyh",
        "outputId": "3256ba6e-1e5b-447d-beb0-db71fffcc574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted value: [5.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q20 :- What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "dw7yoZnoH0Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### In data analysis and machine learning, variables are classified into two main types :-\n",
        "\n",
        "- **1) Continuous Variables**\n",
        "\n",
        "- **2) Categorical Variables**\n",
        "\n",
        "### **Continuous Variables (Quantitative Variables) :-**\n",
        "\n",
        "- Continuous variables are variables that can take any value within a given range, including decimal values. They are numerical and can represent measurements on a scale that allows for infinite possibilities between any two points.\n",
        "\n",
        "- Continuous variables can take an infinite number of values within a given range, and these values can be fractions or decimals.\n",
        "\n",
        "- These variables are typically measurements of some quantity.\n",
        "\n",
        "- You can perform arithmetic operations (addition, subtraction, etc.) on continuous variables meaningfully.\n",
        "\n",
        "- Continuous data is typically represented with real numbers and is often stored as floating-point numbers.\n",
        "\n",
        "- Examples :- Height, Weight, Temperature, Time, etc.\n",
        "\n",
        "### **Categorical Variables (Qualitative Variables) :-**\n",
        "\n",
        "- Categorical variables are variables that take on discrete, distinct categories or labels. These variables can be numerical or non-numerical, but they do not represent a measurable quantity. Instead, they represent a group or category.\n",
        "\n",
        "- Categorical variables can only take a limited number of values (distinct categories or labels).\n",
        "\n",
        "- Categorical variables can be either nominal (no inherent order) or ordinal (with a defined order or ranking).\n",
        "\n",
        "- Arithmetic operations on categorical data are meaningless. You cannot add, subtract, or perform other mathematical operations on categories.\n",
        "\n",
        "- Categorical variables are typically represented as strings, integers (when categories are coded numerically), or binary values for binary classification.\n",
        "\n",
        "- **Types of Categorical Variables :-**\n",
        "\n",
        "- - **Nominal Variables :-** These variables represent distinct categories with no particular order or ranking. Examples :- Gender, Eye Colour, Car, etc.\n",
        "\n",
        "- - **Ordinal Variables :-** These variables have distinct categories that do have a meaningful order or ranking, but the distances between the categories are not defined.- Examples :- Education level, Customer satisfaction, Class ranks, etc.\n"
      ],
      "metadata": {
        "id": "YidQbw4tH5FT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q21 :- What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "0ajsxpbzJ4oW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### Feature scaling is the process of normalizing or standardizing the range of independent variables or features in a dataset. It ensures that each feature contributes equally to the model's performance and helps prevent certain features from dominating others due to differences in their scales (e.g., one feature ranging from 0 to 1 and another ranging from 1 to 1,000,000).\n",
        "\n",
        "\n",
        "### In machine learning, feature scaling is an essential pre-processing step, especially for algorithms that are sensitive to the scale of the data. It helps in bringing all features to the same scale or range so that the model can treat them fairly and learn from the data more effectively.\n",
        "\n",
        "\n",
        "### Feature Scaling Important :-\n",
        "\n",
        "- **Speeds up convergence :-** Helps algorithms converge faster.\n",
        "\n",
        "- **Prevents domination :-** Ensures features with larger values don't dominate the learning process.\n",
        "\n",
        "- **Improves performance of distance-based models :-** Essential for algorithms like k-NN, SVM, and k-Means.\n",
        "\n",
        "- **Helps in regularization :-** Regularization methods like L1 and L2 are sensitive to scale.\n",
        "\n",
        "### Methods of Feature Scaling :-\n",
        "\n",
        "- **1) Min-Max Scaling (Normalization) :-** It transforms data to a fixed range, usually [0, 1].\n",
        "\n",
        "- - It is used when the data has a known fixed range.\n",
        "\n",
        "- **2) Standardization (Z-score Normalization) :-** It transforms data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "- - It is used when data has varying scales and needs a normal distribution.\n",
        "\n",
        "### When to Use Feature Scaling :-\n",
        "\n",
        "- Use scaling with models like k-NN, SVM, Logistic Regression, and Neural Networks.\n",
        "\n",
        "- Don't use scaling with tree-based models like Decision Trees and Random Forests, as they are not sensitive to feature scales."
      ],
      "metadata": {
        "id": "fFLB2ulZJ-F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Min-Max Scaling (Normalization) in Python (using Scikit-learn) :-\n",
        "\n",
        "\n",
        "# Import MinMaxScaler :-\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Create data :-\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize MinMaxScaler :-\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data :-\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d5fw_MxHlud",
        "outputId": "8478e01a-b411-49cb-a477-b6aab23ba335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Standardization (Z-score Normalization) in Python (using Scikit-learn) :-\n",
        "\n",
        "# Import library standardScaler :-\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create data :-\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize StandardScaler :-\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data :-\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy3OmLPSMEUd",
        "outputId": "f6ab4cf4-d0a6-4440-d20e-026de24cb241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q22 :- How do we perform scaling in Python?\n"
      ],
      "metadata": {
        "id": "H9HR_3ll581K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### There are two main techniques to perform scaling in Python :-\n",
        "**1) Standardization** :-\n",
        "\n",
        "- Standardization (Z-score Normalization)\n",
        "It rescales the features so that they have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "- It is recommended when the data is normally distributed or when the model assumes a Gaussian distribution of the features.\n",
        "\n",
        "**2) Normalization** :-\n",
        "\n",
        "- Normalization (Min-Max Scaling) It rescales the data into a specific range, usually between 0 and 1.\n",
        "\n",
        "- It is useful when you need the features to be in a specific range (like for neural networks or when features have different units)."
      ],
      "metadata": {
        "id": "n30UeodX6zeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Standardization using StandardScaler from scikit-learn :-"
      ],
      "metadata": {
        "id": "XyM_fao2sbMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the library :-\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Create a data :-\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize the scaler :-\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler and transform the data :-\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Show the scaled_data :-\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "id": "5wFdrUfmMVxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf3713e-990f-441b-f765-5ed2002b588b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Normalization using MinMaxScaler from scikit-learn :-"
      ],
      "metadata": {
        "id": "yF3Owz--s1z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MinMaxScaler library :-\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a data :-\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Show the scaled_data :-\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAKjqg-Es0R8",
        "outputId": "5ad2d83d-506f-4937-999a-decca96f2496"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q23 :- What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "0-6Ij4fMuSbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### sklearn.preprocessing is a module in scikit-learn used for preparing data before feeding it into machine learning models. It provides tools for scaling, normalizing, encoding categorical data, and handling missing values.\n",
        "\n",
        "### There are some/few common Functions in sklearn.preprocessing :-\n",
        "\n",
        "**1) Scaling and Normalization :-** These are few techniques which is used to adjust the values of numeric features in your dataset, making them more suitable for certain machine learning models.\n",
        "\n",
        "- **StandardScaler :-** Standardizes the features by removing the mean and scaling to unit variance (i.e., Z-score normalization).\n",
        "\n",
        "- **MinMaxScaler :-** Scales features to a given range, usually between 0 and 1.\n",
        "\n",
        "- **QuantileTransformer :-** Transforms the features to follow a uniform or normal distribution.\n",
        "\n",
        "**2) Encoding Categorical Features :-** When working with machine learning models, many algorithms require that all input features be numeric. This module includes tools for converting categorical data into numeric representations.\n",
        "\n",
        "- **LabelEncoder :-** Encodes target labels (class labels) as integers. Useful when you need to convert categorical labels into numerical values.\n",
        "\n",
        "- **OneHotEncoder :-** Converts categorical variables into a one-hot encoded (binary) format. This is useful when you need to convert nominal categorical variables into a format that can be used by machine learning algorithms.\n",
        "\n",
        "- **OrdinalEncoder :-** Encodes categorical features as ordinal integers (for ordinal data, where the categories have an inherent order).\n",
        "\n",
        "**3) Imputation: :-** This technique is used to handle missing values in your dataset by filling them with appropriate values.\n",
        "\n",
        "- **SimpleImputer :-** Fills missing values with mean, median, or mode.\n",
        "\n",
        "**4) Feature Transformation :-** Transforming features helps prepare the data for the learning process, improving performance or meeting model requirements.\n",
        "\n",
        "- **PolynomialFeatures :-** It creates polynomial features.\n",
        "\n",
        "- **PowerTransformer :-** It makes data more Gaussian.\n",
        "\n",
        "\n",
        "### The sklearn.preprocessing is important for :-\n",
        "\n",
        "- Standardization and Scaling\n",
        "- Handling Categorical Data\n",
        "- Improving Model Accuracy\n",
        "- Handling Skewed Data\n"
      ],
      "metadata": {
        "id": "xVyEkoJTuhiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using sklearn.preprocessing :-\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# data :-\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize StandardScaler :-\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data :-\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhEzbQyKtQH-",
        "outputId": "1d9678a2-9cbc-435c-edb4-6751193e0ad9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q24 :- How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "ekzpLIPDxBwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### The most common way to split data in python into :-\n",
        "\n",
        "- training and testing sets is by using the train_test_split() function from the sklearn.model_selection module.\n",
        "\n",
        "### This function splits the dataset into two parts :- **one for training the model** and the other for **testing its performance**."
      ],
      "metadata": {
        "id": "cm4KPI7IxK5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Create data: Features \"input data\" (X) and Labels (y) \"output data\" :-\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "# Split the data into 80% training and 20% testing :-\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training features:-\\n\", X_train)\n",
        "print(\"Testing features:-\\n\", X_test)\n",
        "print(\"Training labels:-\\n\", y_train)\n",
        "print(\"Testing labels:-\\n\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj_-i9hc5xyM",
        "outputId": "f7c18938-71cd-4749-c111-633d01cac4f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features:-\n",
            " [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "Testing features:-\n",
            " [[3 4]]\n",
            "Training labels:-\n",
            " [0 0 0 1]\n",
            "Testing labels:-\n",
            " [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X = features, y = target/labels :-\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRUaywHTwYnK",
        "outputId": "fb30c153-886e-4398-ba9c-a87b18d33a63"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 9, 10],\n",
              "        [ 5,  6],\n",
              "        [ 1,  2],\n",
              "        [ 7,  8]]),\n",
              " array([[3, 4]]),\n",
              " array([0, 0, 0, 1]),\n",
              " array([1]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q25 :- Explain data encoding?"
      ],
      "metadata": {
        "id": "2C4Zg3cF7BD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer :-\n",
        "\n",
        "### Data encoding is the process of converting categorical data into a numerical format so that machine learning algorithms can process it. Most machine learning algorithms require numeric inputs, so encoding is necessary when working with categorical features, which represent categories or labels.\n",
        "\n",
        "### There are some common types of Data Encoding :-\n",
        "\n",
        "**1) Label Encoding :-**\n",
        "\n",
        "- It is suitable for ordinal data, where the categories have an inherent order.\n",
        "\n",
        "**2) One-Hot Encoding :-**\n",
        "\n",
        "- It is ideal for nominal data with no order, creates binary columns.\n",
        "\n",
        "**3) Ordinal Encoding :-**\n",
        "\n",
        "- It is for ordinal data with clear order (like \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "**4) Binary Encoding :-**\n",
        "\n",
        "- It is a more compact way to encode data with many categories, combining label encoding and binary conversion."
      ],
      "metadata": {
        "id": "eyN3y6vq7aN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Label Encoding :-\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "labels = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
        "encoded_labels = le.fit_transform(labels)\n",
        "print(encoded_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QReKi1Kk5pYv",
        "outputId": "92b1677f-ab80-4aa3-8274-fc1aefb0816a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 1 0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of One-Hot Encoding :-\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output = False)\n",
        "colors = np.array(['Red', 'Green', 'Blue', 'Green', 'Red']).reshape(-1, 1)\n",
        "encoded_colors = ohe.fit_transform(colors)\n",
        "print(encoded_colors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8RvUn7e8hwH",
        "outputId": "06bdcc2a-7921-4e67-b933-f03afb20ad56"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Ordinal Encoding :-\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "encoder = OrdinalEncoder()\n",
        "data = [['Low'], ['Medium'], ['High'], ['Medium'], ['Low']]\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVci7oZb87qe",
        "outputId": "bfaabc66-7772-4c1d-c9a9-3704a12f5254"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [2.]\n",
            " [0.]\n",
            " [2.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7kW5VWQ9XnZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}